ğŸ§  Poker AI Project Plan  
Last updated: 2025-07-20  
Author: AiÅ¡a Berro (for ChatGPT continuity)

---

ğŸ¯ Project Goal  
Create a high-performing AI system that learns how to play poker (starting with No-Limit Texas Holdâ€™em) through self-play and reinforcement learning. The system will simulate different AI personalities (agents) and environments to train the ultimate poker decision-maker that can advise the user in real tournaments.

---

ğŸ‘©â€ğŸ’» User Profile  
Name: AiÅ¡a Berro  
Background: Computer Science, Frontend Developer (strong JS, some Python)  
Interest: AI, Reinforcement Learning, Poker  
Motivation: Personal project, not commercial.  
Budget: No limit (but cautious about quality and timing of purchases)

---

ğŸ§  AI Agents  
We are developing 4 separate AI agents, each with its own learning strategy and personality. Each will train and evolve over time:

- **Nashy** â€“ Nash-equilibrium-based conservative agent  
- **Bluffy** â€“ Aggressive, bluffing-heavy agent  
- **Sharky** â€“ Optimal, learning-based self-improver (trained with RL)  
- **Basey** â€“ Rule-based agent with fixed, non-learning strategy (for benchmarking)  
ğŸ†• - **Simon** â€“ (planned) Human-influenced agent that learns from real player Simon via logs or interactions.

Agents will be trained in different environments and game types to evaluate their performance across various contexts (cash games, SNGs, MTTs, etc.).

ğŸ†• A simulated **multi-agent tournament system** is also planned to compare and evolve agents in a competitive setting.

---

ğŸ“ Folder Structure
```bash
poker-ai/
â”œâ”€â”€ agents/          # Each AI agent and base class
â”‚   â”œâ”€â”€ base_agent.py
â”‚   â”œâ”€â”€ nash.py
â”‚   â”œâ”€â”€ bluffy.py
â”‚   â”œâ”€â”€ sharky.py
â”‚   â””â”€â”€ basey.py      # ğŸ†• Rule-based test agent
â”œâ”€â”€ engine/          # Core poker logic
â”‚   â”œâ”€â”€ game.py
â”‚   â”œâ”€â”€ player.py
â”‚   â”œâ”€â”€ cards.py
â”‚   â””â”€â”€ evaluator.py
â”œâ”€â”€ env/             # Gym-compatible training environment
â”‚   â””â”€â”€ poker_env.py
â”œâ”€â”€ train/           # Training scripts
â”‚   â””â”€â”€ train_agents.py
â”œâ”€â”€ utils/           # Logging, helpers, wrappers
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ main.py          # Script to run games or simulations
â”œâ”€â”€ requirements.txt
â””â”€â”€ pytest.ini        # ğŸ†• Testing config for pytest
ğŸ› ï¸ Tech Stack

Python 3.11+

Stable Baselines3

OpenAI Gym

PyPokerEngine / custom engine

Pytest ğŸ†•

VS Code (optional)

macOS (initially on Intel MacBook Pro, upgrades later)

(Possibly later: CUDA/GPU training on cloud or new Mac)

âœ… Current Progress

âœ… Virtualenv created (pokerai)
âœ… Python installed
âœ… Stable Baselines3 installed
âœ… Folder structure scaffolded
âœ… Game engine implemented (cards, game flow, betting)
âœ… hand_evaluator.py working with correct ranking logic
âœ… Showdown display improved with descriptive ranks
âœ… Best 5-card hand shown per player
âœ… basey.py implemented and tested ğŸ†•
âœ… test/test_agents.py created with unit test for Basey ğŸ†•

âŒ RL agents coded
âŒ Training scripts created
âŒ Evaluation/metrics
âŒ Final advisor model

ğŸ“Œ Next Steps

ğŸ†• Design general-purpose RL agent class as foundation for agents like Sharky
ğŸ†• Choose episode-based learning loop to start; consider step-based later
ğŸ†• Modularize reward, observation, and action-space design
ğŸ†• Implement Gym environment in poker_env.py
ğŸ†• Refactor main.py to support modular agents and evaluation
ğŸ†• Use pytest for continuous bug-checking and testing

Define BaseAgent interface in base_agent.py âœ…

Create Basey rule-based agent for benchmarking âœ…

Begin training Sharky (PPO/A2C) via self-play

Evaluate each agent against others in head-to-head play

Continuously refine logic and retrain

Build long-term tournament arena and meta-learning simulation

ğŸ““ Notes

This file is for ChatGPT context restoration in case of lost session/memory.
Paste it at the start of a new session to reestablish context.
Update it as the project evolves.