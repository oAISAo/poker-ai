# ğŸ§  Poker AI Project Plan  
_Last updated: 2025-07-26_  
_Author: AiÅ¡a Berro_

---

## ğŸ¯ Project Goal  
Build a modular, extensible AI system that learns to play No-Limit Texas Holdâ€™em poker through self-play, reinforcement learning, and imitation learning. The system will support multiple agent personalities, robust evaluation, and easy extension for future research and experimentation.

---

## ğŸ‘©â€ğŸ’» User Profile  
- **Name:** AiÅ¡a Berro  
- **Background:** Computer Science, Frontend Developer (JS, Python)  
- **Interest:** AI, RL, Poker  
- **Motivation:** Personal research, not commercial  
- **Budget:** Flexible, focused on quality and learning

---

## ğŸ§  AI Agents  
Develop and evolve multiple agents, each with a unique strategy:

- **Nashy:** Nash-equilibrium conservative agent  
- **Bluffy:** Aggressive, bluffing-focused agent  
- **Sharky:** RL-based self-improver (PPO/A2C)  
- **Basey:** Rule-based benchmark agent  
- **Simon:** Human-influenced agent trained from real hand history

Agents will compete in various environments (cash, SNG, MTT) and a multi-agent tournament system.

---

## ğŸ“ Folder Structure
```bash
poker-ai/
â”œâ”€â”€ agents/          # Agent classes and personalities
â”œâ”€â”€ engine/          # Poker game logic
â”œâ”€â”€ env/             # Gym-compatible environments
â”œâ”€â”€ train/           # Training scripts
â”œâ”€â”€ utils/           # Logging, helpers
â”œâ”€â”€ main.py          # Entry point for games/simulations
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pytest.ini       # Testing config
â””â”€â”€ README.md        # Project documentation
```

---

## ğŸ› ï¸ Tech Stack
- Python 3.11+
- Stable Baselines3, sb3-contrib
- OpenAI Gym/Gymnasium
- PyPokerEngine / custom engine
- Pytest
- VS Code
- macOS (Intel/Apple Silicon)
- (Optional: CUDA/GPU/cloud training)

---

## âœ… Current Progress
- Virtualenv and dependencies set up
- Folder structure scaffolded
- Poker engine implemented (cards, betting, hand evaluation)
- Base agents and rule-based logic tested
- RL agent (Sharky) training and saving implemented
- Evaluation and metrics logging in place

---

## ğŸ“Œ Next Steps
1. **Agent Evolution:**  
   - Train and version Sharky (`sharky_1.0.0`, `sharky_1.0.1`, ...), run tournaments, and evolve best agents.
2. **Human Imitation Agent:**  
   - Parse Simonâ€™s hand history, train `Simon_basic` via supervised learning, then evolve with RL.
3. **Mixed Tournaments:**  
   - Run tournaments with all agent versions, log and analyze results.
4. **Documentation & Usability:**  
   - Add docstrings and usage examples to all public classes/functions.
   - Update `README.md` with setup, training, evaluation, and extension instructions.
5. **Testing & Validation:**  
   - Expand unit tests and integration tests for all modules.
6. **Visualization:**  
   - Plot training/evaluation metrics for agent progress tracking.

---

## ğŸš€ Best Practices
- Save models and logs with clear versioning.
- Visualize training and tournament results.
- Periodically retrain and evaluate agents against each other.
- Use both RL and supervised learning for robust agent development.
- Maintain up-to-date documentation and onboarding guides.

---

## ğŸ““ Notes
- This file is for project context and continuity.
- Update regularly as the project evolves.
- Use as a reference for onboarding, planning, and retrospectives.